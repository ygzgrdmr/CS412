# -*- coding: utf-8 -*-
"""sample_experiment_notebook.ipynb adlı not defterinin kopyası

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/188gZenqVdQhgMyHnTzuTDT2vbN8cvWSc

# 0. Initialize

## 0.1. Import Libraries
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %load_ext autoreload
# %autoreload 2
import os, sys, glob
import gzip
import random
import tqdm
import json
import numpy as np
import pandas as pd
pd.set_option("display.max_columns", None)

from IPython import display
import matplotlib as mpl
from matplotlib import pyplot as plt

"""## 0.2. DEFINE VARIABLES """

# Commented out IPython magic to ensure Python compatibility.
DATA_PATH = '/content/drive/MyDrive/cs412/' # '<insert-your-training-data-path-here>'
# %ls

ROUND = 1 # This project will have 3 rounds of predictions: 1,2,3
STUDENT_ID = '22534'#'<insert-your-id-here>'
PROJECT_CODE = 'CS412cb12d847a4ee'#'<insert-your-code-here>' # Same code for the annotation eg. CS412xxxxx

from google.colab import drive
drive.mount('/content/drive')

"""## 0.3. Read Training & Evaluation Data

### 0.3.1. Get the labels for tweets
"""

# Given training
trainingTweetDf = pd.read_csv('{}training-tweet.csv'.format(DATA_PATH), dtype={'tweet_id': str, 'isPolitical': str})

# My annotations
my_training_tweet = pd.read_csv(('{}annotated_tweets_'+PROJECT_CODE+'.csv').format(DATA_PATH), dtype={'Unnamed: 0': str, 'isPolitical': str})

my_training_tweet.drop(['url', 'sentiment', 'isExperiential', 'isInsult'], 1, inplace=True)
my_training_tweet.rename(columns = {'Unnamed: 0':'tweet_id'}, inplace=True)

# Concatenating -> droping duplicates -> reindexing
trainingTweetDf = pd.concat([my_training_tweet, trainingTweetDf], axis='rows')
trainingTweetDf.drop_duplicates(subset=['tweet_id'], inplace=True)
trainingTweetDf.reset_index(drop=True, inplace=True)

trainingTweetDf

trainingTweetDf.isPolitical.value_counts()

"""### 0.3.2. Get the labels for users"""

# Given training
trainingUserDf = pd.read_csv('{}training-user.csv'.format(DATA_PATH), dtype={'screen_name': str, 'isBot': str})

# My annotations
my_training_user = pd.read_csv(('{}annotated_users_'+PROJECT_CODE+'.csv').format(DATA_PATH), dtype={'Unnamed: 0': str, 'isBot': str})

my_training_user.drop(['url', 'isOrganizational', 'isTroll'], 1, inplace=True)
my_training_user.rename(columns = {'Unnamed: 0':'screen_name'}, inplace=True)

# Concatenating -> droping duplicates -> reindexing
trainingUserDf = pd.concat([my_training_user, trainingUserDf], axis='rows')
trainingUserDf.drop_duplicates(subset=['screen_name'], inplace=True)
trainingUserDf.reset_index(drop=True, inplace=True)

trainingUserDf

trainingUserDf.isBot.value_counts()

"""### 0.3.3. Expand your dataset with metadata and tweets"""

# You can also expand training data by downloading your own labeled datasets following the link
# Download the documents under "Link to training data"

print('http://www.onurvarol.com/Annotation-CS412-202201/reports/report_{}.html'.format(PROJECT_CODE))

"""# 1. EXTRACT FEATURES
Under *1.1. Political Tweet Detection* and *1.2. Bot Detection*, we firstly collect raw data for processing. We then combine some of them (total_interactions = num_favorites + num_retweets) or use them to extract features (whether the tweet has one of the political entities @meralaksener, @kilicdarogluk etc.).

We expect you to collect more raw data from **tweet_metadata**, **user_profiles** and **user_tweets** files by creating a function as shown in below examples such as *check_if_retweet()* and using it while iterating over data as shown under *Merge Collected Features*.

We also expect you to create new variables as much as you can from the data in order to make your predictions more accurate. For example, you may want to check:

- The tweet sources that a user frequently uses
- Whether the user is a verified account or not

...

to assess whether **a user is a bot or not** and whether **a tweet is political or not**.
"""

PATH_TO_DOWNLOADED = DATA_PATH # 'D:/Users/suuser/Desktop/Sabancı/CS412/spring-2022/project/'

"""## 1.1. Political Tweet Detection
This part stands for the feature extraction of tweets. We start with collecting the raw data from *tweet_metadata*, then use some of them to extract features.

### 1.1.1. Get Raw Data

#### 1.1.1.1. Check if Retweet
"""

def check_if_retweet(tweet_metadata_line):
    is_retweet = 0
    retweeted_username = None

    try:
        tweet_metadata_line['retweeted_status']
        retweeted_username = tweet_metadata_line['retweeted_status']['user']['screen_name'].lower()
        is_retweet = 1

    except KeyError:
        pass

    return is_retweet, retweeted_username

"""#### 1.1.1.2. Get Tweet Text"""

def get_tweet_text(tweet_metadata_line):
    text = tweet_metadata_line['text']
    
    return text

"""#### 1.1.1.3. Get Tweet ID"""

def get_tweet_id(tweet_metadata_line):
    id_str = tweet_metadata_line['id_str']
    
    return id_str

"""#### 1.1.1.4. Get Number of Mentions and Hashtags"""

def get_number_mentions_hashtags(tweet_metadata_line):
    num_mentions = len(tweet_metadata_line['entities']['user_mentions'])
    num_hashtags = len(tweet_metadata_line['entities']['hashtags'])

    return num_mentions, num_hashtags

"""#### 1.1.1.5. Get Number of Retweets and Favorites"""

def get_number_retweets_favorites(tweet_metadata_line):
    retweet_count = tweet_metadata_line['retweet_count']
    favorite_count = tweet_metadata_line['favorite_count']
    
    return retweet_count, favorite_count

"""#### 1.1.1.6. Get User Info"""

def get_user_info(tweet_metadata_line):
    id = tweet_metadata_line['user']['id_str']
    screen_name = tweet_metadata_line['user']['screen_name'].lower()
    description = tweet_metadata_line['user']['description']

    return id, screen_name, description

"""### 1.1.2. Derive Manually Crafted Features

#### 1.1.2.1. Check for political entity in text
"""

def check_political_ent(text, checkIn):
    text = text.lower()

    # These keywords have been drastically reduced due to public access from github!
    # Possible keywords that can be found in political tweets.
    tweet_entities = [' ak ', 'akp', 'chp', 'hdp', 'mhp', 'iyi part', "ak parti" ]


    # These keywords have been drastically reduced due to public access from github!
    # Possible keywords that can be found in the profile descriptions of people who regularly tweet politically.    
    desc_entities = [' ak ', 'akp', 'chp', 'mhp', 'hdp', 'iyi part',  "ak parti"]


    # These keywords have been drastically reduced due to public access from github!
    # Has this tweet been retweeted by people who mostly tweeted politically?
    retweet_from_entities = ['meral_aksener', 'kilicdarogluk', 'vekilince', 'RTE', 'rte', "recep tayyip", 'tayyip' ]
                        

    entities_in_text = []

    if checkIn == 'retweetedFrom':
        entities_in_text = [ent for ent in retweet_from_entities if ent.lower() in text]
        if len(entities_in_text) >= 1:
            return 1

    elif checkIn == 'tweet':
      entities_in_text = [ent for ent in retweet_from_entities+tweet_entities if ent.lower() in text]

    elif checkIn == 'desc':
      entities_in_text = [ent for ent in desc_entities if ent.lower() in text]

    return len(entities_in_text)

"""#### 1.1.2.2. Number of total interactions"""

def total_interactions(retweet_count, favorite_count):
    total_num_interactions = retweet_count + favorite_count
    
    return total_num_interactions

def political_word_total_word_ratio(text):
    num_political_entities = check_political_ent(text, 'tweet')
    total_word = len(set(text.split()))

    return num_political_entities/total_word

"""### 1.1.2. Collect data using the functions above and transform into a Pandas DataFrame"""

dfPolitical = {'tweet_id':[],
              'is_retweet':[],
              'retweeted_username':[],
              'text':[],
              'num_mentions':[],
              'num_hashtags':[],
              'num_retweets':[],
              'num_favorites':[],
              'user_id':[],
              'user_screen_name':[],
              'user_description':[],           
              'num_political_entities_tweet':[],
              'num_political_entities_description': [],
              'retweeted_political': [],
              'political_word_ratio': [],             
              'total_interactions':[]}


with gzip.open(f"{PATH_TO_DOWNLOADED}tweet_metadata.jsons.gz", "rb") as f:
    for line in f:
        line = json.loads(line)
        
        # raw data:
        id_str = get_tweet_id(line)
        is_retweet, retweeted_username = check_if_retweet(line)
        text = get_tweet_text(line)
        num_mentions, num_hashtags = get_number_mentions_hashtags(line)
        retweet_count, favorite_count = get_number_retweets_favorites(line)
        user_id_str, screen_name, user_description = get_user_info(line)
  
        # manually crafted data:
        total_num_interactions = total_interactions(retweet_count, favorite_count)
        retweeted_political = bool(check_political_ent(retweeted_username, 'retweetedFrom')) if is_retweet else False
        num_political_entities = check_political_ent(text, 'tweet')
        num_political_entities_in_description = check_political_ent(user_description, 'desc')
        political_word_ratio = political_word_total_word_ratio(text)


        dfPolitical['tweet_id'].append(id_str)
        dfPolitical['is_retweet'].append(is_retweet)
        dfPolitical['retweeted_username'].append(retweeted_username)
        dfPolitical['text'].append(text)
        dfPolitical['num_mentions'].append(num_mentions)
        dfPolitical['num_hashtags'].append(num_hashtags)
        dfPolitical['num_retweets'].append(retweet_count)
        dfPolitical['num_favorites'].append(favorite_count)
        dfPolitical['user_id'].append(user_id_str)
        dfPolitical['user_screen_name'].append(screen_name)
        dfPolitical['user_description'].append(user_description)
        dfPolitical['total_interactions'].append(total_num_interactions)
        dfPolitical['num_political_entities_tweet'].append(num_political_entities)
        dfPolitical['num_political_entities_description'].append(num_political_entities_in_description)
        dfPolitical['retweeted_political'].append(retweeted_political)
        dfPolitical['political_word_ratio'].append(political_word_ratio)

dfPolitical = pd.DataFrame(dfPolitical)
dfPolitical

"""## 1.2. From Users

### 1.2.1. Get user metadata from user_profiles.jsons.gz

#### 1.2.1.1. Get user info metadata
"""

def get_user_info_metadata(user_metadata_line):
    
    user_id = user_metadata_line['id_str']
    user_name = user_metadata_line['name']
    user_screen_name = user_metadata_line['screen_name'].lower()
    user_location = user_metadata_line['location']
    user_description = user_metadata_line['description']
    user_followers_count = user_metadata_line['followers_count']
    user_friends_count = user_metadata_line['friends_count']  
    user_is_verified = user_metadata_line['verified']
    user_has_default_photo = user_metadata_line['default_profile_image']
    user_statuses_count = user_metadata_line['statuses_count']

    
    dictionary = {'user_id':user_id, 'user_name': user_name, 'user_screen_name':user_screen_name, 'user_location':user_location,
     'user_description':user_description, 'user_followers_count':user_followers_count, 'user_friends_count':user_friends_count, 
     'user_statuses_count': user_statuses_count, 'user_is_verified':user_is_verified, 'user_has_default_photo': user_has_default_photo}

    return dictionary

"""#### 1.2.1.2. Get followers/(followers+friends) ratio"""

def get_followers_all_ratio(user_followers_count, user_friends_count):
    
    if user_friends_count + user_followers_count == 0:
        followers_all_ratio = 0

    else:
        followers_all_ratio =  user_followers_count / (user_friends_count + user_followers_count)

    return followers_all_ratio

"""#### 1.2.1.3. Get description length"""

def get_desc_len(user_description):
    
    description_len = len(user_description)

    return description_len

dfBot = {'user_id':[],
         'user_name':[],
         'user_screen_name':[],
         'user_location':[],
         'user_description':[],
         'user_followers_count':[],
         'user_friends_count':[],
         'user_statuses_count':[],
         'description_len':[],
         'followers_to_all_ratio':[],
         'user_is_verified':[],
         'user_has_default_photo': []
         }

with gzip.open(f"{PATH_TO_DOWNLOADED}user_profiles.jsons.gz", "rb") as f:
    for line in f:
        line = json.loads(line)

        dictionary = get_user_info_metadata(line)
        for k,v in dictionary.items():
            dfBot[k].append(v)

        
        # manually crafted data:
        description_len = get_desc_len(dictionary['user_description'])
        dfBot['description_len'].append(description_len)
        
        followers_all_ratio = get_followers_all_ratio(dictionary['user_followers_count'], 
                                                      dictionary['user_friends_count'])

        dfBot['followers_to_all_ratio'].append(followers_all_ratio)

dfBot = pd.DataFrame(dfBot)
dfBot

"""### 1.2.2. Get Tweet Info of Users in user_profiles.jsons.gz

#### 1.2.2.1. Check ratio of retweets to all tweets
"""

def get_retweet_tweet_ratio(line):
    number_retweets = 0
    number_original_tweets = 0

    for tweet in line['tweets']:
        try:
            tweet['retweeted_status']
            number_retweets += 1
                
        except:
            number_original_tweets += 1
            
    total_tweets = number_retweets + number_original_tweets
    
    if total_tweets == 0:
        retweet_total_ratio = None
    else:
        retweet_total_ratio = number_retweets/(total_tweets)
    
    return retweet_total_ratio

"""#### 1.2.2.2. Check median number of favorites"""

def get_median_number_favorites(line):
    num_median_favorites = np.median([tweet['favorite_count'] for tweet in line['tweets']])

    return num_median_favorites

"""### 1.2.3. Collect data using the functions above and transform into a Pandas DataFrame"""

dfBotTweets = {'user_id':[],
               'retweet_total_ratio':[],
               'num_median_favorites':[],
               'num_of_tweets':[]
              }

i = 0

with gzip.open(f"{PATH_TO_DOWNLOADED}user_tweets.jsons.gz", "rb") as f:
    for line in f:

        line = json.loads(line)

        user_id = line['user_id']
        dfBotTweets['user_id'].append(user_id)
        
        retweet_total_ratio = get_retweet_tweet_ratio(line)
        dfBotTweets['retweet_total_ratio'].append(retweet_total_ratio)
        
        num_median_favorites = get_median_number_favorites(line)
        dfBotTweets['num_median_favorites'].append(num_median_favorites)
        
        dfBotTweets['num_of_tweets'].append(len(line['tweets']))

        i += 1
        if i % 1000 == 0:
            print(i)

dfBotTweets = pd.DataFrame(dfBotTweets)
dfBotTweets

"""### 1.2.3. Merge dfBot and dfBotTweets"""

dfBotAll = dfBot.merge(dfBotTweets,
                       how='left')

dfBotAll[['retweet_total_ratio', 'num_median_favorites']] = dfBotAll[['retweet_total_ratio', 'num_median_favorites']].fillna(0)

dfBotAll

"""# 2. TRAIN MODEL

## 2.1. Political Tweet Prediction

### 2.1.1. Merge dfPolitical data with labels
"""

dfPoliticalAll_train = dfPolitical.merge(trainingTweetDf,
                                         on='tweet_id')

dfPoliticalAll_train.head()

"""### 2.1.2. Separate X and y values
We only use 3 features here to create a baseline model. However, it is not enough to get good results.
"""

X = dfPoliticalAll_train[['num_political_entities_tweet', 'num_political_entities_description', 'retweeted_political', 'num_mentions', 'total_interactions', 'num_hashtags', 'political_word_ratio']]
y = dfPoliticalAll_train['isPolitical'].apply(lambda x: 1 if x=='Yes' else 0)

"""### 2.1.3. Train - validation split"""

from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

"""### 2.1.4. Train the model

Here, you may use different models such as neural networks, XGBoost, AdaBoost, RandomForest, Linear Regression, Logistic Regression etc. to see which model does the best. Also, you can use grid_search_cv() or a basic for loop to optimize the hyperparameters of your model.
"""

# Hyper parameter optimization for isPolitical
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix, mean_squared_error, make_scorer
import xgboost as xgb

mse = make_scorer(mean_squared_error, greater_is_better=False)
params = {
    'max_depth': range(3, 10, 2),
    'min_child_weight':range(1,6,2),
    'subsample':[i/10.0 for i in range(6,10)],
    'colsample_bytree':[i/10.0 for i in range(6,10)],
    'gamma':[i/10.0 for i in range(0,5)]
}

# create an instance
xgb_reg = xgb.XGBRegressor(
    objective='binary:logistic'
)

# grid search the model
grid_search_political = GridSearchCV(estimator = xgb_reg, param_grid= params, n_jobs = 4, cv = 5, verbose = True, scoring=mse)

# fit your model
grid_search_political.fit(X_train, y_train)

grid_search_political.best_estimator_

# make predictions
preds = grid_search_political.predict(X_valid)

# evaluate on validation set
mse_political = mean_squared_error(y_valid, preds)

print("MSE:", mse_political)

"""## 2.2. Bot Detection

### 2.2.1. Merge dfBotAll data with labels
"""

dfBotAll.user_screen_name = dfBotAll.user_screen_name.str.lower()

dfBotAll_train = dfBotAll.merge(trainingUserDf,
                               left_on='user_screen_name',
                               right_on='screen_name')

dfBotAll_train

trainingUserDf.isBot.value_counts()

"""### 2.2.2. Separate X and y values
We use only 4 features here to create a baseline model. However, it is not enough to get good results.
"""

X = dfBotAll_train[['description_len', 'followers_to_all_ratio', 'retweet_total_ratio', 'num_median_favorites']]
y = dfBotAll_train.isBot.apply(lambda x: 1 if x=='Yes' else 0)

"""### 2.2.3. Train-test split"""

from sklearn.model_selection import train_test_split

X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)

"""### 2.2.4. Train the model"""

# Hyper parameter optimization for isBot
mse = make_scorer(mean_squared_error, greater_is_better=False)
params = {
    'max_depth': range(3, 10, 2),
    'min_child_weight':range(1,6,2),
    'subsample':[i/10.0 for i in range(6,10)],
    'colsample_bytree':[i/10.0 for i in range(6,10)],
    'gamma':[i/10.0 for i in range(0,5)]
}

# create an instance
xgb_reg = xgb.XGBRegressor(
    objective='binary:logistic'
)

# grid search the model
grid_search_bot = GridSearchCV(estimator = xgb_reg, param_grid= params, n_jobs = 4, cv = 5, verbose = True, scoring=mse)

# fit your model
grid_search_bot.fit(X_train, y_train)

grid_search_bot.best_estimator_

# make predictions
preds = grid_search_bot.predict(X_valid)

# evaluate on validation set
mse_bot = mean_squared_error(y_valid, preds)

print("MSE:", mse_bot)

"""# 3. MAKE PREDICTIONS

Here, you will make predictions with the models that you have trained above.

## 3.1. Predictions for Tweets (Political or Not)
"""

# read the evaluation file as follows
evaluationTweetDf = pd.read_csv('{}evaluation-round1-tweet.csv'.format(DATA_PATH), dtype={0: str}, header=None, names=['tweet_id'])
evaluationTweetDf = evaluationTweetDf.dropna()

# merge it with the political dataframe so that you can use the make predictions based on the variables
dfPolitical_test = dfPolitical.merge(evaluationTweetDf)

# define X as we did above in section (2.x.2. Separate X and y values)
X = dfPolitical_test[['num_political_entities_tweet', 'num_political_entities_description', 'retweeted_political', 'num_mentions', 'total_interactions', 'num_hashtags', 'political_word_ratio']]

# make predictions based on these variables
predictions_political = grid_search_political.predict(X)

"""### This part is important! We expect you to return your predictions in the following format:"""

modelPredTweet = dict([(x,float(y)) for x,y in zip([*dfPolitical_test.tweet_id], predictions_political)])
modelPredTweet

"""## 3.2. Predictions for Users (Bot or Not)"""

evaluationUserDf = pd.read_csv('{}evaluation-round1-user.csv'.format(DATA_PATH), dtype={0: str}, header=None, names=['user_screen_name'])
evaluationUserDf = evaluationUserDf.dropna()

# merge it with the political dataframe so that you can use the make predictions based on the variables
dfBot_test = dfBotAll.merge(evaluationUserDf)

# define X as we did above in section (2.x.2. Separate X and y values)
X = dfBot_test[['description_len', 'followers_to_all_ratio', 'retweet_total_ratio', 'num_median_favorites']]

# make predictions based on these variables
predictions_bot = grid_search_bot.predict(X)

modelPredUser = dict([(x,float(y)) for x,y in zip([*dfBot_test.user_screen_name], predictions_bot)])
modelPredUser

"""# PREPARE SUBMISSION

You will need to submit exact same file produced by using the following code. Any deviation from the desired format willbe marked as 0.
"""

# Explain your approach

data_explanations = '''
To explain how I handle data for training, I can include information about the following:

Data acquisition: First of all, I downloaded the data that I had annotated from Onur teacher's website and installed it,
then added additional data. I read these from csv file also I concatenated and droped duplicates finally reindexed the data

Data cleaning: I droped duplicate values
Data splitting: for is political x training values: 'num_political_entities_tweet', 'num_political_entities_description', 'retweeted_political', 'num_mentions', 'total_interactions', 'num_hashtags', 'political_word_ratio'
y training value obviously isPolitical 
and for bot detection x training values 'description_len', 'followers_to_all_ratio', 'num_median_favorites', 'user_friends_count', 'user_statuses_count', 'user_followers_count', 'user_has_default_photo', 'user_is_verified', 'num_retweet']]
y training value is isBot
'''

feature_explanations = '''
The code you provided appears to define a grid search cross-validation (CV) procedure for hyperparameter optimization using the XGBoost library. The XGBoost model is a gradient boosting model for binary classification, as indicated by the objective parameter set to 'binary:logistic'. The model is trained to predict whether a tweet is political or not (isPolitical).

The features used to train the model are:

'num_political_entities_tweet': The number of political entities in the tweet text.
'num_political_entities_description': The number of political entities in the user's description.
'retweeted_political': A binary feature indicating whether the tweet is a political retweet.
'num_mentions': The number of user mentions in the tweet.
'total_interactions': The total number of interactions (likes and retweets) the tweet has received.
'num_hashtags': The number of hashtags used in the tweet.
'political_word_ratio': The ratio of political words to total words in the tweet.
These features are used to predict the target variable 'isPolitical', which is a binary variable indicating whether the tweet is political (1) or not (0). The model is trained using a train-test split of the data, with 80% of the data used for training and 20% used for validation. The grid search procedure searches a defined parameter grid to find the optimal combination of hyperparameters for the model, using the mean squared error (MSE) as the evaluation metric. The best estimator, or the model with the optimal combination of hyperparameters, is then stored in the best_estimator_ attribute of the grid search object.
'''

model_explanations = '''
The code I provided to define a grid search cross-validation (CV) procedure for hyperparameter optimization using the XGBoost library.

The XGBoost model used in this code is a gradient boosting model for binary classification, as indicated by the objective parameter set to 'binary:logistic'. The model is trained to predict whether an example is a bot or not (isBot).
'''

additional_explanations = '''
I did not use any trick but next time I will be change test_size or random_state with different values
'''

predictions = {
    'round': ROUND,
    'student_id': STUDENT_ID,
    'user_predictions': modelPredUser,
    'tweet_predictions': modelPredTweet,
    'explanations': {
        'data': data_explanations,
        'feature': feature_explanations,
        'model': model_explanations,
        'other': additional_explanations,
    }
}


with open('predictions-{}_round{}.json'.format(STUDENT_ID, ROUND), 'w') as fl:
    fl.write(json.dumps(predictions, indent=4))

# Test your submission file

submission = json.load(open('predictions-{}_round{}.json'.format(STUDENT_ID, ROUND), 'r'))
submission

